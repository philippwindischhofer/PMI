{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=8, inter_op_parallelism_threads=8, \\\n",
    "                        allow_soft_placement=True, device_count = {'CPU': 8})\n",
    "sess = tf.InteractiveSession(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computes the KL divergence D_KL(P||Q) between the distributions of P and Q, \n",
    "# given an equal number of samples drawn from them\n",
    "def KL_network(data_P, data_Q, name):\n",
    "    with tf.variable_scope(name):  \n",
    "        data_combined = tf.concat([data_P, data_Q], axis = 0)\n",
    "        \n",
    "        lay = layers.relu(data_combined, 20)\n",
    "        lay = layers.relu(lay, 20)\n",
    "        outputs = layers.linear(lay, 1)\n",
    "        \n",
    "    these_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = name)\n",
    "    \n",
    "    return outputs, these_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KL_loss(KL_output, name):\n",
    "    with tf.variable_scope(name):\n",
    "        batch_size_dyn = tf.cast(tf.math.divide(tf.shape(KL_output)[0], 2), tf.int32)\n",
    "        \n",
    "        T_P = KL_output[:batch_size_dyn,:]\n",
    "        T_Q = KL_output[batch_size_dyn:,:]\n",
    "        \n",
    "        TF_loss = -(tf.reduce_mean(T_P, axis = 0) - tf.math.log(tf.reduce_mean(tf.math.exp(T_Q), axis = 0)))\n",
    "        \n",
    "        TF_loss = TF_loss[0]\n",
    "        \n",
    "    return TF_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_P_in = tf.placeholder(tf.float32, [None, 1], name = 'data_P_in')\n",
    "data_Q_in = tf.placeholder(tf.float32, [None, 1], name = 'data_Q_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KL_output, KL_vars = KL_network(data_P_in, data_Q_in, name = 'KL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KL_lossval = KL_loss(KL_output, 'KL_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_KL = tf.train.AdamOptimizer(learning_rate = 0.005, beta1 = 0.3, beta2 = 0.5).minimize(KL_lossval, var_list = KL_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_Q_train = np.random.normal(loc = 0, scale = 1, size = 50000)\n",
    "data_P_train = np.random.normal(loc = 1, scale = 1, size = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_Q_train = np.expand_dims(data_Q_train, axis = 1)\n",
    "data_P_train = np.expand_dims(data_P_train, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 0.5036814212799072\n",
      "epoch 1: loss = 0.5039948225021362\n",
      "epoch 2: loss = 0.5043094158172607\n",
      "epoch 3: loss = 0.5043565034866333\n",
      "epoch 4: loss = 0.5043433904647827\n",
      "epoch 5: loss = 0.5039451122283936\n",
      "epoch 6: loss = 0.503258228302002\n",
      "epoch 7: loss = 0.5041403770446777\n",
      "epoch 8: loss = 0.5043749809265137\n",
      "epoch 9: loss = 0.5043420791625977\n",
      "epoch 10: loss = 0.5040558576583862\n",
      "epoch 11: loss = 0.5034643411636353\n",
      "epoch 12: loss = 0.5040665864944458\n",
      "epoch 13: loss = 0.5043784379959106\n",
      "epoch 14: loss = 0.5044025182723999\n",
      "epoch 15: loss = 0.5043841600418091\n",
      "epoch 16: loss = 0.5039399862289429\n",
      "epoch 17: loss = 0.5031805038452148\n",
      "epoch 18: loss = 0.5042284727096558\n",
      "epoch 19: loss = 0.5044219493865967\n",
      "epoch 20: loss = 0.5043220520019531\n",
      "epoch 21: loss = 0.5039271116256714\n",
      "epoch 22: loss = 0.5036430358886719\n",
      "epoch 23: loss = 0.5042078495025635\n",
      "epoch 24: loss = 0.5044084787368774\n",
      "epoch 25: loss = 0.5044209957122803\n",
      "epoch 26: loss = 0.5043830871582031\n",
      "epoch 27: loss = 0.5037965774536133\n",
      "epoch 28: loss = 0.5034236907958984\n",
      "epoch 29: loss = 0.5043138265609741\n",
      "epoch 30: loss = 0.5044467449188232\n",
      "epoch 31: loss = 0.504375696182251\n",
      "epoch 32: loss = 0.5040487051010132\n",
      "epoch 33: loss = 0.5034826993942261\n",
      "epoch 34: loss = 0.5041524171829224\n",
      "epoch 35: loss = 0.5044399499893188\n",
      "epoch 36: loss = 0.5044456720352173\n",
      "epoch 37: loss = 0.5043963193893433\n",
      "epoch 38: loss = 0.5037291049957275\n",
      "epoch 39: loss = 0.5035805702209473\n",
      "epoch 40: loss = 0.5043399333953857\n",
      "epoch 41: loss = 0.5044722557067871\n",
      "epoch 42: loss = 0.5044164657592773\n",
      "epoch 43: loss = 0.5042141675949097\n",
      "epoch 44: loss = 0.5033445358276367\n",
      "epoch 45: loss = 0.5040897130966187\n",
      "epoch 46: loss = 0.5044764280319214\n",
      "epoch 47: loss = 0.5044522285461426\n",
      "epoch 48: loss = 0.5043147802352905\n",
      "epoch 49: loss = 0.5035796165466309\n",
      "epoch 50: loss = 0.503901481628418\n",
      "epoch 51: loss = 0.5044268369674683\n",
      "epoch 52: loss = 0.5044775009155273\n",
      "epoch 53: loss = 0.5044722557067871\n",
      "epoch 54: loss = 0.5043150186538696\n",
      "epoch 55: loss = 0.5031170845031738\n",
      "epoch 56: loss = 0.5040931701660156\n",
      "epoch 57: loss = 0.5044952630996704\n",
      "epoch 58: loss = 0.5044314861297607\n",
      "epoch 59: loss = 0.5042306184768677\n",
      "epoch 60: loss = 0.5036909580230713\n",
      "epoch 61: loss = 0.5040191411972046\n",
      "epoch 62: loss = 0.5044344663619995\n",
      "epoch 63: loss = 0.5044947862625122\n",
      "epoch 64: loss = 0.5045008659362793\n",
      "epoch 65: loss = 0.5043891668319702\n",
      "epoch 66: loss = 0.503258466720581\n",
      "epoch 67: loss = 0.5039774179458618\n",
      "epoch 68: loss = 0.5045063495635986\n",
      "epoch 69: loss = 0.5044716596603394\n",
      "epoch 70: loss = 0.5042324066162109\n",
      "epoch 71: loss = 0.503720760345459\n",
      "epoch 72: loss = 0.5040949583053589\n",
      "epoch 73: loss = 0.5044437646865845\n",
      "epoch 74: loss = 0.5045076608657837\n",
      "epoch 75: loss = 0.504452109336853\n",
      "epoch 76: loss = 0.5039141178131104\n",
      "epoch 77: loss = 0.5035356283187866\n",
      "epoch 78: loss = 0.5043714046478271\n",
      "epoch 79: loss = 0.5045299530029297\n",
      "epoch 80: loss = 0.5045009851455688\n",
      "epoch 81: loss = 0.5042634010314941\n",
      "epoch 82: loss = 0.5034517049789429\n",
      "epoch 83: loss = 0.5041368007659912\n",
      "epoch 84: loss = 0.5045226812362671\n",
      "epoch 85: loss = 0.5045320987701416\n",
      "epoch 86: loss = 0.5044542551040649\n",
      "epoch 87: loss = 0.5037480592727661\n",
      "epoch 88: loss = 0.5037676095962524\n",
      "epoch 89: loss = 0.5044267177581787\n",
      "epoch 90: loss = 0.5045346021652222\n",
      "epoch 91: loss = 0.5045338869094849\n",
      "epoch 92: loss = 0.5044658184051514\n",
      "epoch 93: loss = 0.5037004947662354\n",
      "epoch 94: loss = 0.5036827325820923\n",
      "epoch 95: loss = 0.5044533014297485\n",
      "epoch 96: loss = 0.5045374631881714\n",
      "epoch 97: loss = 0.504433274269104\n",
      "epoch 98: loss = 0.5039279460906982\n",
      "epoch 99: loss = 0.5037930011749268\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    sess.run(train_KL, feed_dict = {data_P_in: data_P_train, data_Q_in: data_Q_train})\n",
    "    loss = -sess.run(KL_lossval, feed_dict = {data_P_in: data_P_train, data_Q_in: data_Q_train})\n",
    "    print(\"epoch {}: loss = {}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
